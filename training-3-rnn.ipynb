{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('deeplearning': conda)"
  },
  "interpreter": {
   "hash": "a630840cc17be2676cbce8189d2083744dc2f6895f9cfddc5644aa2e06831cd0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, input_size)\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        embedding = self.embedding(input_seq)\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_size = 512   # size of hidden state\n",
    "seq_len = 100       # length of LSTM sequence\n",
    "num_layers = 3      # num of layers in LSTM layer stack\n",
    "lr = 0.002          # learning rate\n",
    "epochs = 3          # max number of epochs\n",
    "op_seq_len = 9    # total num of characters in output test sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'00000000011001000'"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "def int_to_binary(num, width):\n",
    "    if num >= 0:\n",
    "        binary = bin(num)[2:].zfill(width)\n",
    "        return \"0\" + binary \n",
    "    else:\n",
    "        binary = bin(abs(num))[2:].zfill(width)\n",
    "        return \"1\" + binary\n",
    "\n",
    "int_to_binary(200, 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "def binary_to_int(str, width):\n",
    "    sign = str[0]\n",
    "    num = str[1:]\n",
    "    num = int(num, 2)\n",
    "\n",
    "    if sign == \"0\":\n",
    "        return num\n",
    "    else:\n",
    "        return num * - 1\n",
    "\n",
    "binary_to_int('00000000011001000', 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([list(['00000000000000001', '00000000000000010', '00000000000000001', '00000000000000101', '00000000000000101', '00000000000000001', '00000000000001011', '00000000000010000', '00000000000000111', '00000000000000001', '00000000000010111', '00000000000101100', '00000000000011110', '00000000000001001', '00000000000000001', '00000000000101111', '00000000001110000', '00000000001101000', '00000000000110000', '00000000000001011', '00000000000000001', '00000000001011111', '00000000100010000', '00000000101000000', '00000000011001000', '00000000001000110', '00000000000001101', '00000000000000001', '00000000010111111', '00000001010000000', '00000001110010000', '00000001011010000', '00000000101010100', '00000000001100000', '00000000000001111', '00000000000000001', '00000000101111111', '00000010111000000', '00000100110100000', '00000100100110000', '00000010101111000', '00000001000010100', '00000000001111110', '00000000000010001', '00000000000000001', '00000001011111111', '00000110100000000', '00001100100000000', '00001110000000000', '00001010000100000', '00000100110100000', '00000001100010000', '00000000010100000', '00000000000010011', '00000000000000001', '00000010111111111', '00001110100000000']),\n",
       "       list(['00000000000000001', '00000000000001000', '00000000000011001', '00000000001010011', '00000000100010010', '00000100011100011', '0100000010010000000', '0100000010110011100011', '01100101000101100101001', '0101001111100111001011110', '010001010011111011001000011', '010001111101111000000001110110', '010000010100001111010001000011111111', '010000010110011111000000000100001101110', '0110011000101111110111010010101001001001', '010101001101000011011101111000011101001001', '01000101111111111010010000111100000000100100', '01001000101001100010101100101011000100001101001', '01000001111110011001011011110001001001010011111101110', '01000010000111011110101000000110101110101100011111011001']),\n",
       "       list(['00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000101', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000101', '00000000000000101', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000001011', '00000000000000101', '00000000000000101', '00000000000001011', '00000000000000101', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000101', '00000000000010111', '00000000000000101', '00000000000010111', '00000000000000101', '00000000000000101', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000010101', '00000000000000101', '00000000000100111', '00000000000000101', '00000000000000101', '00000000000100111', '00000000000000101', '00000000000010101', '00000000000000101', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000101', '00000000000000001', '00000000000010001', '00000000000000001', '00000000000010001', '00000000000000001', '00000000000000001', '00000000000000101', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000000001', '00000000000011111', '00000000000000101', '00000000000000101', '00000000000011101', '00000000000000001', '00000000000000001', '00000000000011101', '00000000000000001', '00000000000000101']),\n",
       "       ...,\n",
       "       list(['00000000000000000', '00000000000000001', '00000000000001001', '00000000001010101', '00000001100100001', '00001110101111101', '010001010111101001', '010100011101100100101', '011000000110101111110001', '011100011001011100000001101', '0100001011101000010100000111001', '0100111011010010000100101000110101', '0101110011011010110110011110011000001', '0110110101100011010101100100101110011101', '01000000011011101011011101011001101110001001', '01001011111001111010100011110111010011001000101', '01011001011010111000100110001011110100011010010001', '01101001010101110111010101001100110000001010000101101', '01111100000110010001110010001111000101011100111111011001', '010010010001100011011011101110101110001001001001111101010101', '010101100001110011000000000101101011011101000001101100101100001', '011001010111000111100101110101010011110001001110100010000110111101']),\n",
       "       list(['00000000000000010', '00000000000000011', '00000000000000011', '00000000000000100', '00000000000000110', '00000000000000100', '00000000000000101', '00000000000001010', '00000000000001010', '00000000000000101', '00000000000000110', '00000000000001111', '00000000000010100', '00000000000001111', '00000000000000110', '00000000000000111', '00000000000010101', '00000000000100011', '00000000000100011', '00000000000010101', '00000000000000111', '00000000000001000', '00000000000011100', '00000000000111000', '00000000001000110', '00000000000111000', '00000000000011100', '00000000000001000', '00000000000001001', '00000000000100100', '00000000001010100', '00000000001111110', '00000000001111110', '00000000001010100', '00000000000100100', '00000000000001001', '00000000000001010', '00000000000101101', '00000000001111000', '00000000011010010', '00000000011111100', '00000000011010010', '00000000001111000', '00000000000101101', '00000000000001010', '00000000000001011', '00000000000110111', '00000000010100101', '00000000101001010', '00000000111001110', '00000000111001110', '00000000101001010', '00000000010100101', '00000000000110111', '00000000000001011', '00000000000001100', '00000000001000010', '00000000011011100', '00000000111101111', '00000001100011000', '00000001110011100', '00000001100011000', '00000000111101111', '00000000011011100', '00000000001000010', '00000000000001100', '00000000000001101', '00000000001001110', '00000000100011110', '00000001011001011']),\n",
       "       list(['00000000000000101', '00000000000000111', '00000000010110011', '00000000011100101', '00000000110110111', '00000001000101101', '00001011111100001', '0110110101011111111'])],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "\n",
    "\n",
    "# load the text file\n",
    "\n",
    "data = pd.read_csv('data/preprocessed-3-rnn.csv')\n",
    "data = data[data.columns[0]].apply(ast.literal_eval)\n",
    "data = data.apply(lambda seq: [int_to_binary(int(num), 16) for num in seq])\n",
    "# data = data[data.columns[0]].str.split(',').to_numpy()\n",
    "# data = [[int(num) for num in row] for row in data]\n",
    "\n",
    "# data = data[data.columns[0]]\n",
    "# data = [[int(num) for num in row] for row in data]\n",
    "# # print(type(data[data.columns[0]].str.cat(sep = '')))\n",
    "# chars = sorted(list(set(data)))\n",
    "# # print(chars)\n",
    "# data_size, vocab_size = len(data), len(chars)\n",
    "# print(\"----------------------------------------\")\n",
    "# print(\"Data has {} characters, {} unique\".format(data_size, vocab_size))\n",
    "# print(\"----------------------------------------\")\n",
    "data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bin_symbols = '01'\n",
    "bin_n_symbols = len(all_bin_symbols)\n",
    "\n",
    "def binToIndex(letter):\n",
    "    return all_bin_symbols.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def binToTensor(letter):\n",
    "    tensor = torch.zeros(1, len(letter))\n",
    "    for i, ch in enumerate(letter):\n",
    "        tensor[:,i][binToIndex(letter)] = int(ch)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "n = '00000000000000101'\n",
    "binToTensor(n).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    " torch.zeros(1, 10)[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-f1ac6934356c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# data tensor on device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# data tensor on device\n",
    "data = torch.tensor(data).to(device)\n",
    "data = torch.unsqueeze(data, dim=1)\n",
    "\n",
    "# model instance\n",
    "rnn = RNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "66\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not list",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-6c795d3532ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-9999af7b5414>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, hidden_state)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/deeplearning/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1911\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for i_epoch in range(1, epochs+1):\n",
    "    \n",
    "    # random starting point (1st 100 chars) from data to begin\n",
    "    data_ptr = np.random.randint(100)\n",
    "    n = 0\n",
    "    running_loss = 0\n",
    "    hidden_state = None\n",
    "    \n",
    "    while True:\n",
    "        print(data_ptr)\n",
    "        input_seq = data[data_ptr : data_ptr+seq_len]\n",
    "        target_seq = data[data_ptr+1 : data_ptr+seq_len+1]\n",
    "        \n",
    "        # forward pass\n",
    "        output, hidden_state = rnn(input_seq, hidden_state)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # compute gradients and take optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update the data pointer\n",
    "        data_ptr += seq_len\n",
    "        n +=1\n",
    "        \n",
    "        # if at end of data : break\n",
    "        if data_ptr + seq_len + 1 > data_size:\n",
    "            break\n",
    "        \n",
    "    # print loss and save weights after every epoch\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(i_epoch, running_loss/n))\n",
    "    \n",
    "    # sample / generate a text sequence after every epoch\n",
    "    data_ptr = 0\n",
    "    hidden_state = None\n",
    "    \n",
    "    # random character from data to begin\n",
    "    rand_index = np.random.randint(data_size-1)\n",
    "    input_seq = data[rand_index : rand_index+1]\n",
    "    \n",
    "    print(\"----------------------------------------\")\n",
    "    while True:\n",
    "        # forward pass\n",
    "        output, hidden_state = rnn(input_seq, hidden_state)\n",
    "        \n",
    "        # construct categorical distribution and sample a character\n",
    "        output = F.softmax(torch.squeeze(output), dim=0)\n",
    "        dist = Categorical(output)\n",
    "        index = dist.sample()\n",
    "        \n",
    "        # print the sampled character\n",
    "        print(ix_to_char[index.item()], end='')\n",
    "        \n",
    "        # next input is current output\n",
    "        input_seq[0][0] = index.item()\n",
    "        data_ptr += 1\n",
    "        \n",
    "        if data_ptr > op_seq_len:\n",
    "            break\n",
    "        \n",
    "    print(\"\\n----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}