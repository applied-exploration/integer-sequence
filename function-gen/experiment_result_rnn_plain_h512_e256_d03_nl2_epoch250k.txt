Experiment 1: Training size: 10k, Batch size: 32, Hidden: 512, Embedding: 256, Dropout: 0.3, No of hidden layers: 2, Epochs: 250k, 
EncoderRNN(
  (embedding): Embedding(4300, 256)
  (gru): GRU(256, 512, num_layers=2, dropout=0.3)
)
DecoderRNN(
  (embedding): Embedding(16, 256)
  (gru): GRU(256, 512, num_layers=2, dropout=0.3)
  (out): Linear(in_features=512, out_features=16, bias=True)
  (softmax): LogSoftmax(dim=1)
)
21m 4s (- 189m 37s) (25000 10%) 1.3985
42m 32s (- 170m 11s) (50000 20%) 1.1524
64m 0s (- 149m 22s) (75000 30%) 0.9672
85m 26s (- 128m 9s) (100000 40%) 0.8704
106m 53s (- 106m 53s) (125000 50%) 0.8157
128m 20s (- 85m 33s) (150000 60%) 0.7811
149m 47s (- 64m 11s) (175000 70%) 0.7607
171m 14s (- 42m 48s) (200000 80%) 0.7479
192m 43s (- 21m 24s) (225000 90%) 0.7362
214m 16s (- 0m 0s) (250000 100%) 0.7288
Accuracy score on training set:  0.4849
Accuracy score on test set:  0.3557084288791606
