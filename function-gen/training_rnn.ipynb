{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.rnn.rnn_plain import RNN_Plain\n",
    "from models.rnn.rnn_attention import RNN_Attention\n",
    "\n",
    "from lang import load_data_int_seq\n",
    "from utils import accuracy_score\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lang, input_lang, train, X_test, y_test = load_data_int_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR DATALOADER\r\n",
    "# output_lang, input_lang, train_data_iterable, test_data_iterable = load_data_int_seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(2564, 28)\n",
      "  (gru): GRU(28, 256, num_layers=2, dropout=0.3)\n",
      ")\n",
      "DecoderRNN(\n",
      "  (embedding): Embedding(16, 28)\n",
      "  (gru): GRU(28, 256, num_layers=2, dropout=0.3)\n",
      "  (out): Linear(in_features=256, out_features=16, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "algo = RNN_Plain(symbols = \"+*-0123456789t\", \r\n",
    "output_sequence_length = 9, \r\n",
    "encoded_seq_length = 9, \r\n",
    "num_epochs = 15000, \r\n",
    "input_size = input_lang.n_words, \r\n",
    "hidden_size = 256, \r\n",
    "output_size=output_lang.n_words, \r\n",
    "embedding_size = 28, \r\n",
    "batch_size = 32, \r\n",
    "num_gru_layers = 2,\r\n",
    "dropout_prob = 0.3,\r\n",
    "calc_magnitude_on=False)\r\n",
    "\r\n",
    "# algo.__class__ = RNN_Plain\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 29s (- 13m 26s) (1500 10%) 1.9896\n",
      "3m 1s (- 12m 6s) (3000 20%) 1.5195\n",
      "4m 34s (- 10m 40s) (4500 30%) 1.5175\n",
      "6m 5s (- 9m 8s) (6000 40%) 1.5162\n",
      "7m 35s (- 7m 35s) (7500 50%) 1.5144\n",
      "9m 6s (- 6m 4s) (9000 60%) 1.5144\n",
      "10m 37s (- 4m 33s) (10500 70%) 1.5144\n",
      "12m 7s (- 3m 1s) (12000 80%) 1.5134\n",
      "13m 38s (- 1m 30s) (13500 90%) 1.5141\n",
      "15m 29s (- 0m 0s) (15000 100%) 1.5125\n"
     ]
    }
   ],
   "source": [
    "# algo.train(input_lang, output_lang, train_data_iterable) \r\n",
    "algo.train(input_lang, output_lang, train)\r\n",
    "\r\n",
    "# from line_profiler import LineProfiler\r\n",
    "\r\n",
    "# lp = LineProfiler()\r\n",
    "# lp_wrapper = lp(algo.train)\r\n",
    "# lp_wrapper(input_lang, output_lang, train)\r\n",
    "# lp.print_stats()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor_minibatch  torch.Size([9, 32])\n",
      "input_tensor  torch.Size([9, 32])\n",
      "torch.Size([2, 32, 256])\n",
      "[tensor([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "         4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0'), tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3,\n",
      "         3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:0'), tensor([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "         4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0'), tensor([[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "         5, 5, 5, 5, 5, 5, 5, 5]], device='cuda:0'), tensor([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "         4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0'), tensor([[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "         5, 5, 5, 5, 5, 5, 5, 5]], device='cuda:0'), tensor([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "         4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0'), tensor([[5, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 5, 3, 3, 5, 5, 3, 3, 3, 3, 3, 5, 3, 5,\n",
      "         5, 3, 3, 3, 3, 3, 3, 5]], device='cuda:0'), tensor([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "         4, 4, 4, 4, 4, 4, 4, 4]], device='cuda:0'), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')]\n",
      "concatenated_output_sequences  torch.Size([32, 10])\n",
      "tensor([[4, 3, 4, 5, 4, 5, 4, 5, 4, 0],\n",
      "        [4, 3, 4, 5, 4, 5, 4, 3, 4, 0],\n",
      "        [4, 3, 4, 5, 4, 5, 4, 3, 4, 0],\n",
      "        [4, 3, 4, 5, 4, 5, 4, 5, 4, 0],\n",
      "        [4, 3, 4, 5, 4, 5, 4, 3, 4, 0],\n",
      "        [4, 3, 4, 5, 4, 5, 4, 3, 4, 0],\n",
      "        [4, 3, 4, 5, 4, 5, 4, 3, 4, 0],\n",
      "        [4, 3, 4, 5, 4, 5, 4, 3, 4, 0],\n",
      "        [4, 3, 4, 5, 4, 5, 4, 3, 4, 0],\n",
      "        [4, 3, 4, 5, 4, 5, 4, 3, 4, 0]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t*t+t+t+t',\n",
       " 't*t+t+t*t',\n",
       " 't*t+t+t*t',\n",
       " 't*t+t+t+t',\n",
       " 't*t+t+t*t',\n",
       " 't*t+t+t*t',\n",
       " 't*t+t+t*t',\n",
       " 't*t+t+t*t',\n",
       " 't*t+t+t*t',\n",
       " 't*t+t+t*t']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = algo.infer(input_lang, output_lang, X_test[:1000])\r\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred, y_test[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain RNN with Magnitude loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magn_algo = RNN_Plain(symbols = \"+*-0123456789t\", output_sequence_length = 9, encoded_seq_length = 9, num_epochs= 5000, input_size=input_lang.n_words, hidden_size=512, output_size=output_lang.n_words, calc_magnitude_on=True)\n",
    "magn_algo.train(input_lang, output_lang, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = magn_algo.infer(input_lang, output_lang, X_test[:1000])\r\n",
    "pred[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred, y_test[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Â Attention-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_attn = RNN_Attention(symbols = \"+*-0123456789t\", output_sequence_length = 9, encoded_seq_length = 9, num_epochs= 5000, input_size=input_lang.n_words, hidden_size=512, output_size=output_lang.n_words, calc_magnitude_on=False)\n",
    "algo_attn.train(input_lang, output_lang, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = algo_attn.infer(input_lang, output_lang, X_test[:1000])\r\n",
    "pred[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred, y_test[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention with magnitude loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_attn_magn = RNN_Attention(symbols = \"+*-0123456789t\", output_sequence_length = 9, encoded_seq_length = 9, num_epochs= 5000, input_size=input_lang.n_words, hidden_size=512, output_size=output_lang.n_words, calc_magnitude_on=True)\n",
    "algo_attn_magn.train(input_lang, output_lang, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = algo_attn_magn.infer(input_lang, output_lang, X_test[:1000])\r\n",
    "pred[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred, y_test[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.stdout": [
       "tensor([[-0.2686,  0.0357, -1.1628],\n",
       "        [ 0.3468,  0.3241,  2.1244]])\n",
       "tensor([[-1.0169, -0.7126, -1.9111],\n",
       "        [-2.0660, -2.0887, -0.2884]])\n"
      ]
     },
     "output_type": "unknown"
    },
    {
     "data": {
      "application/vnd.code.notebook.stderr": [
       "ipykernel_launcher:6: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "m = nn.LogSoftmax()\r\n",
    "input = torch.randn(2, 3)\r\n",
    "print(input)\r\n",
    "output = m(input)\r\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.stdout": [
       "output:  torch.Size([5, 4, 8, 8])\n",
       "target:  torch.Size([5, 8, 8])\n"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "# 2D loss example (used, for example, with image inputs)\r\n",
    "N, C = 5, 4\r\n",
    "loss = nn.NLLLoss()\r\n",
    "\r\n",
    "# input is of size N x C x height x width\r\n",
    "data = torch.randn(N, 16, 10, 10)\r\n",
    "conv = nn.Conv2d(16, C, (3, 3))\r\n",
    "m = nn.LogSoftmax(dim=1)\r\n",
    "\r\n",
    "# print(data.shape)\r\n",
    "activated = m(conv(data))\r\n",
    "print(\"output: \", activated.shape)\r\n",
    "\r\n",
    "# each element in target has to have 0 <= value < C\r\n",
    "target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\r\n",
    "print(\"target: \", target.shape)\r\n",
    "output = loss(activated, target)\r\n",
    "\r\n",
    "# output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.stdout": [
       "torch.Size([3, 5])\n",
       "torch.Size([3])\n"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "m = nn.LogSoftmax(dim=1)\r\n",
    "loss = nn.NLLLoss()\r\n",
    "# input is of size N x C = 3 x 5\r\n",
    "# input = minibatchsize, number of categories\r\n",
    "\r\n",
    "input = torch.randn(3, 5, requires_grad=True)\r\n",
    "# print(input.shape)\r\n",
    "activation = m(input)\r\n",
    "print(activation.shape)\r\n",
    "# each element in target has to have 0 <= value < C\r\n",
    "target = torch.tensor([1, 0, 4])\r\n",
    "print(target.shape)\r\n",
    "output = loss(activation, target)\r\n",
    "# output.backward()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21b14e6ae810b98687c42101764fbc6ed2f749f10b37141b42930ea65db4f89b"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('drlnd': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}