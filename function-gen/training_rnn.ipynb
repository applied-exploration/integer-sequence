{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('deeplearning': conda)"
  },
  "interpreter": {
   "hash": "a630840cc17be2676cbce8189d2083744dc2f6895f9cfddc5644aa2e06831cd0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.rnn.rnn_plain import RNN_Plain\n",
    "from models.rnn.rnn_attention import RNN_Attention\n",
    "\n",
    "from lang import load_data_int_seq\n",
    "from utils import accuracy_score\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lang, input_lang, train, X_test, y_test = load_data_int_seq()"
   ]
  },
  {
   "source": [
    "## Plain RNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0m 37s (- 5m 38s) (500 10%) 1.7521\n",
      "1m 15s (- 5m 1s) (1000 20%) 1.6211\n",
      "1m 55s (- 4m 29s) (1500 30%) 1.5479\n",
      "2m 36s (- 3m 54s) (2000 40%) 1.5272\n",
      "3m 16s (- 3m 16s) (2500 50%) 1.5369\n",
      "3m 54s (- 2m 36s) (3000 60%) 1.5019\n",
      "4m 33s (- 1m 57s) (3500 70%) 1.5215\n",
      "5m 12s (- 1m 18s) (4000 80%) 1.5030\n",
      "5m 52s (- 0m 39s) (4500 90%) 1.5059\n",
      "6m 30s (- 0m 0s) (5000 100%) 1.4831\n"
     ]
    }
   ],
   "source": [
    "algo = RNN_Plain(symbols = \"+*-0123456789t\", output_sequence_length = 9, encoded_seq_length = 9, num_epochs= 5000, input_size=input_lang.n_words, hidden_size=512, output_size=output_lang.n_words, calc_magnitude_on=False)\n",
    "algo.train(train, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['t*t*t+t-t',\n",
       " 't*t*t*t*t',\n",
       " 't+t+t-t+t',\n",
       " 't*t*t*t-t',\n",
       " 't*t*t*t*t',\n",
       " 't-t-t-t-t',\n",
       " 't-t+t-t+t',\n",
       " 't*t*t+t+t',\n",
       " 't+t+t-t+t',\n",
       " 't-t-t-t-t',\n",
       " 't*t*t*t*t',\n",
       " 't*t+t+t+t',\n",
       " 't*t*t*t+t',\n",
       " 't*t*t+t+t',\n",
       " 't+t+t-t+t',\n",
       " 't*t*t*t+t',\n",
       " 't-t*t+t-t',\n",
       " 't+t+t+t+t',\n",
       " 't*t*t+t-t',\n",
       " 't+t+t+t+t',\n",
       " 't*t+t+t+t',\n",
       " 't+t-t+t+t',\n",
       " 't-7-t-t-t',\n",
       " 't-t-t-t*t',\n",
       " 't-7-t-t-t']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "pred = algo.infer(X_test[:1000], input_lang, output_lang)\n",
    "pred[:25]"
=======
    "algo.train(input_lang, output_lang, train)"
>>>>>>> 45b2993... feat: Created RNN-GA architecture. Now needs to be tested.
=======
    "algo.train(input_lang, output_lang, train)"
>>>>>>> fa59ed37274c20d04a907ae3933daf7b176b2f23
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred, y_test[:1000])"
   ]
  },
  {
   "source": [
    "### Plain RNN with Magnitude loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "magn_algo = RNN_Plain(symbols = \"+*-0123456789t\", output_sequence_length = 9, encoded_seq_length = 9, num_epochs= 5000, input_size=input_lang.n_words, hidden_size=512, output_size=output_lang.n_words, calc_magnitude_on=True)\n",
    "magn_algo.train(train, input_lang, output_lang)"
=======
=======
>>>>>>> fa59ed37274c20d04a907ae3933daf7b176b2f23
    "pred = algo.infer(input_lang, output_lang, X_test[:100])\n",
    "print(pred)"
>>>>>>> 45b2993... feat: Created RNN-GA architecture. Now needs to be tested.
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = magn_algo.infer(X_test[:1000], input_lang, output_lang)\n",
    "pred[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred, y_test[:1000])"
   ]
  },
  {
   "source": [
    "## Â Attention-based"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_attn = RNN_Attention(symbols = \"+*-0123456789t\", output_sequence_length = 9, encoded_seq_length = 9, num_epochs= 5000, input_size=input_lang.n_words, hidden_size=512, output_size=output_lang.n_words, calc_magnitude_on=False)\n",
    "algo_attn.train(train, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = algo_attn.infer(X_test[:1000], input_lang, output_lang)\n",
    "pred[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred, y_test[:1000])"
   ]
  },
  {
   "source": [
    "### Attention with magnitude loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "algo_attn_magn = RNN_Attention(symbols = \"+*-0123456789t\", output_sequence_length = 9, encoded_seq_length = 9, num_epochs= 5000, input_size=input_lang.n_words, hidden_size=512, output_size=output_lang.n_words, calc_magnitude_on=True)\n",
    "algo_attn_magn.train(train, input_lang, output_lang)"
=======
    "magn_algo.train(input_lang, output_lang, train)"
>>>>>>> 45b2993... feat: Created RNN-GA architecture. Now needs to be tested.
=======
    "magn_algo.train(input_lang, output_lang, train)"
>>>>>>> fa59ed37274c20d04a907ae3933daf7b176b2f23
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "pred = algo_attn_magn.infer(X_test[:1000], input_lang, output_lang)\n",
    "pred[:25]"
=======
=======
>>>>>>> fa59ed37274c20d04a907ae3933daf7b176b2f23
    "pred = magn_algo.infer(input_lang, output_lang, X_test[:1000])\n",
    "#print(pred)"
>>>>>>> 45b2993... feat: Created RNN-GA architecture. Now needs to be tested.
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pred, y_test[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}